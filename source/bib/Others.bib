%%%%
% 只是引用一下
%%%%
@inproceedings{BertAnalysis_21_AAAI_India,
  title={The heads hypothesis: A unifying statistical approach towards understanding multi-headed attention in bert},
  author={Pande, Madhura and Budhraja, Aakriti and Nema, Preksha and Kumar, Pratyush and Khapra, Mitesh M},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  pages={13613--13621},
  year={2021}
}
@article{AttentionIsAllYouNeed,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  pages={1--11},
}
@inproceedings{vig2019analyzing,
    title = "Analyzing the Structure of Attention in a Transformer Language Model",
    author = "Vig, Jesse  and
      Belinkov, Yonatan",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    pages = "63--76",
}
@article{LLMblackbox_lipton2018,
  title={The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery.},
  author={Lipton, Zachary C},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018},
  publisher={ACM New York, NY, USA}
}
@inproceedings{LLMblackbox_gilpin2018,
  title={Explaining explanations: An overview of interpretability of machine learning},
  author={Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  booktitle={2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)},
  pages={80--89},
  year={2018},
  organization={IEEE}
}
@article{BERT_MODEL,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={Preprint at arXiv},
  year={2018},
  note={\href{https://doi.org/10.48550/arXiv.1810.04805}{https://doi.org/10.48550/arXiv.1810.04805}},
}
@article{GPT2_MODEL,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{DNN_Interp_Montavon,
  title={Methods for interpreting and understanding deep neural networks},
  author={Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Digital signal processing},
  volume={73},
  pages={1--15},
  year={2018},
  publisher={Elsevier}
}
@inproceedings{liu-etal-2020-understanding,
    title = "Understanding the Difficulty of Training Transformers",
    author = "Liu, Liyuan and Liu, Xiaodong  and
      Gao, Jianfeng and Chen, Weizhu  and Han, Jiawei",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    pages = "5747--5763",
}
@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}
@article{ScalingLaw_20_arXiv_OpenAI,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={Preprint at arXiv},
  year={2020},
  note={\href{https://doi.org/10.48550/arXiv.2001.08361}{https://doi.org/10.48550/arXiv.2001.08361}},
}
@article{ScalingLaw_21_arXiv_Stanford,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={Preprint at arXiv},
  year={2021},
  note={\href{https://doi.org/10.48550/arXiv.2108.07258}{https://doi.org/10.48550/arXiv.2108.07258}},
}
@article{ICSF_24_arXiv_IAAR,
  title={Internal Consistency and Self-Feedback in Large Language Models: A Survey},
  author={Liang, Xun and Song, Shichao and Zheng, Zifan and Wang, Hanyu and Yu, Qingchen and Li, Xunkai and Li, Rong-Hua and Xiong, Feiyu and Li, Zhiyu},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2407.14507}{https://doi.org/10.48550/arXiv.2407.14507}},
}
@article{GLU_2020_arXiv_Google,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={Preprint at arXiv},
  year={2020},
  note={\href{https://doi.org/10.48550/arXiv.2002.05202}{https://doi.org/10.48550/arXiv.2002.05202}},
}
@article{ROPE_24_Neuro_Zhuiyi,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}
@inproceedings{FewShotLearners,
    title = "Language Models are Few-shot Multilingual Learners",
    author = "Winata, Genta Indra and Madotto, Andrea and Lin, Zhaojiang and Liu, Rosanne and Yosinski, Jason and Fung, Pascale",
    booktitle = "Proceedings of the 1st Workshop on Multilingual Representation Learning",
    year = "2021",
    publisher = "Association for Computational Linguistics",
    pages = "1--15",
}
@article{MOETransformer,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}
@article{ICLDisentagle_23_thesis,
    title={What In-Context Learning “Learns” In-Context: Disentangling Task Recognition and Task Learning}, 
    author={Pan, Jane and Gao, Tianyu and Chen, Howard and Chen, Danqi},
    year={2023},
    journal={Preprint at arXiv},
    note={\href{https://doi.org/10.48550/arXiv.2305.09731}{https://doi.org/10.48550/arXiv.2305.09731}},
}
@article{xFinder_24_arXiv_IAAR,
  title={xFinder: Robust and Pinpoint Answer Extraction for Large Language Models},
  author={Yu, Qingchen and Zheng, Zifan and Song, Shichao and Li, Zhiyu and Xiong, Feiyu and Tang, Bo and Chen, Ding},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2405.11874}{https://doi.org/10.48550/arXiv.2405.11874}},
}
@article{ji2024llm,
  title={LLM Internal States Reveal Hallucination Risk Faced With a Query},
  author={Ji, Ziwei and Chen, Delong and Ishii, Etsuko and Cahyawijaya, Samuel and Bang, Yejin and Wilie, Bryan and Fung, Pascale},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2407.03282}{https://doi.org/10.48550/arXiv.2407.03282}}
}
@inproceedings{DeadNeuron_24_ACL_Meta,
    title = "Neurons in Large Language Models: Dead, N-gram, Positional",
    author = "Voita, Elena and Ferrando, Javier and Nalmpantis, Christoforos",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    year = "2024",
    publisher = "Association for Computational Linguistics",
    pages = "1288--1301",
}

@InProceedings{LinearRepresent_24_PMLR_UChicago,
  title = {The Linear Representation Hypothesis and the Geometry of Large Language Models},
  author = {Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  pages = {39643--39666},
  year = {2024},
  volume = {235},
  publisher = {PMLR},
}
@inproceedings{BertInterp_19_ACL,
    title = "Revealing the Dark Secrets of {BERT}",
    author = "Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    pages = "4365--4374",
}
@inproceedings{BertMarkov_19_ACL,
    title = "{BERT} has a Mouth, and It Must Speak: {BERT} as a {M}arkov Random Field Language Model",
    author = "Wang, Alex and Cho, Kyunghyun",
    booktitle = "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    pages = "30--36",
}
@article{GenderBias_20_NIPS_Salesforce,
    title={Investigating gender bias in language models using causal mediation analysis},
    author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
    journal={Advances in neural information processing systems},
    volume={33},
    pages={12388--12401},
    year={2020}
}
@misc{Githubrepo,
  title={Reference list for the paper “Attention Heads of Large Language Models”},
  author={Zheng, Zifan and Wang, Yezhaohui and Huang, Yuxin and Song, Shichao and Yang, Mingchuan and Tang, Bo and Xiong, Feiyu and Li, Zhiyu},
  howpublished={Zenodo},
  year={2024},
  note={\href{https://doi.org/10.5281/zenodo.14601922}{https://doi.org/10.5281/zenodo.14601922}}
}