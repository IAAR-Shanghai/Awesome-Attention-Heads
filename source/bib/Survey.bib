%%%%
% Survey
%%%%
@inproceedings{SurveyDNNInner_23_SaTML_MIT,
  title={Toward transparent ai: A survey on interpreting the inner structures of deep neural networks},
  author={R{\"a}uker, Tilman and Ho, Anson and Casper, Stephen and Hadfield-Menell, Dylan},
  booktitle={2023 ieee conference on secure and trustworthy machine learning (satml)},
  pages={464--483},
  year={2023},
  organization={IEEE}
}
@article{SurveyNeurAttn_21_arXiv_Brazil,
  title={Neural attention models in deep learning: Survey and taxonomy},
  author={Santana, Alana and Colombini, Esther},
  journal={Preprint at arXiv},
  year={2021},
  note={\href{https://doi.org/10.48550/arXiv.2112.05909}{https://doi.org/10.48550/arXiv.2112.05909}},
}
@article{SurveyMdedical_22_IEEE_Portugal,
  title={A survey on attention mechanisms for medical applications: are we moving toward better Algorithms?},
  author={Gon{\c{c}}alves, Tiago and Rio-Torto, Isabel and Teixeira, Lu{\'\i}s F and Cardoso, Jaime S},
  journal={IEEE Access},
  volume={10},
  pages={98909--98935},
  year={2022},
  publisher={IEEE}
}
@article{SurveyAttentionModel_21_Linkedin,
  title={An attentive survey of attention models},
  author={Chaudhari, Sneha and Mithal, Varun and Polatkan, Gungor and Ramanath, Rohan},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={12},
  number={5},
  pages={1--32},
  year={2021},
  publisher={ACM New York, NY}
}
@inproceedings{SurveyNLPAttn_18_arXiv_GIT,
  title={An introductory survey on attention mechanisms in NLP problems},
  author={Hu, Dichao},
  booktitle={Intelligent Systems and Applications: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys) Volume 2},
  pages={432--448},
  year={2020},
  organization={Springer}
}
@article{SurveydDLAttn_22_arixv_Netherland,
  title={A general survey on attention mechanisms in deep learning},
  author={Brauwers, Gianni and Frasincar, Flavius},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={35},
  number={4},
  pages={3279--3298},
  year={2021},
  publisher={IEEE}
}
@article{SurveyLLMInterp_24_arXiv,
  title={From understanding to utilization: A survey on explainability for large language models},
  author={Luo, Haoyan and Specia, Lucia},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2401.12874}{https://doi.org/10.48550/arXiv.2401.12874}},
}
@inproceedings{SurveyNLPExplain_20_ACL_IBM,
    title = "A Survey of the State of Explainable {AI} for Natural Language Processing",
    author = "Danilevsky, Marina and Qian, Kun and Aharonov, Ranit and Katsis, Yannis and Kawas, Ban and Sen, Prithviraj",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    pages = "447--459",
}
@article{SurveyCTG_24_arXiv_RUC,
  title={Controllable Text Generation for Large Language Models: A Survey},
  author={Liang, Xun and Wang, Hanyu and Wang, Yezhaohui and Song, Shichao and Yang, Jiawei and Niu, Simin and Hu, Jie and Liu, Dan and Yao, Shunyu and Xiong, Feiyu and others},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2408.12599}{https://doi.org/10.48550/arXiv.2408.12599}},
}
@article{SurveyMOE_24_arXiv_HKUST,
  title={A survey on mixture of experts},
  author={Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2407.06204}{https://doi.org/10.48550/arXiv.2407.06204}}
}