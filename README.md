<h2 align='center'> Awesome-Attention-Heads </h2>
<div align='center'>

[![Awesome Attention Heads](https://img.shields.io/static/v1?label=&message=Awesome+Attention+Heads&color=black&logo=awesomelists)](https://github.com/IAAR-Shanghai/Awesome-Attention-Heads) ![](https://img.shields.io/github/last-commit/IAAR-Shanghai/Awesome-Attention-Heads?color=green)

</div>

Welcome to **Awesome-Attention-Heads**! This is the platform to get the latest research on Attention Heads. We hope to provide complete and clear cutting-edge informations for researchers studying LLM interpretability and LLM hallucination.

### Background
With the development of large language models, their underlying network structure, the Transformer, is being extensively studied. Researching the Transformer structure helps us enhance our understanding of this "black box" and improve model interpretability. Recently, there has been an increasing body of work suggesting that the model contains two distinct partitions: attention mechanisms used for behavior, inference, and analysis, and feed-forward networks (FFN) for knowledge storage. The former is crucial for revealing the functional capabilities of the model, leading to a series of studies exploring various functions within attention mechanisms, which we have termed **Attention Head Mining**.

### Table of Contents
- [Latest Papers](#lastest-papers)

### Lastest Papers
Papers below are ordered by publication date:

| Date | Paper | Tags | Links & Summary |
| --- | --- | --- | --- |
| 2024-06-16 | **Induction Heads as a Primary Mechanism for Pattern Matching in In-context Learning** | ![](https://img.shields.io/badge/Induction%20Head-blue) | [![Paper](https://img.shields.io/badge/OpenReview-Paper-%23D2691E)](https://openreview.net/forum?id=np6hrTv7aW) |
| 2024-06-04 | **Iteration Head: A Mechanistic Study of Chain-of-Thought** | ![](https://img.shields.io/badge/Iteration%20Head-blue) | [![Paper](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2406.02128) |
| 2024-05-28 | **Knowledge Circuits in Pretrained Transformers** | ![](https://img.shields.io/badge/Mover%20Head-blue) ![](https://img.shields.io/badge/Relation%20Head-blue) | [![Paper](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2405.17969) [![Code](https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github)](https://github.com/zjunlp/KnowledgeCircuits) |
| 2024-04-24 | **Retrieval Head Mechanistically Explains Long-Context Factuality** | ![](https://img.shields.io/badge/Retrieval%20Head-blue) | [![Paper](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2404.15574) [![Code](https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github)](https://github.com/nightdessert/Retrieval_Head) |
| 2024-04-16 | **Information Flow Routes: Automatically Interpreting Language Models at Scale** | ![](https://img.shields.io/badge/Positional%20Head-blue)![](https://img.shields.io/badge/Subword%20merging%20Head-blue) | [![Paper](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2403.00824) [![Code](https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github)](https://github.com/facebookresearch/llm-transparency-tool) |
| 2024-03-27 | **Non-Linear Inference Time Intervention: Improving LLM Truthfulness** | ![](https://img.shields.io/badge/Truthfulness%20Head-blue)  | [![Paper](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2403.18680) [![Code](https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github)](https://github.com/Samsung/NL-ITI) |
| 2024-02-28 | **Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models** | ![](https://img.shields.io/badge/Memory%20Head-blue)![](https://img.shields.io/badge/Context%20Head-blue) | [![Paper](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2402.18154) |
| 2024-02-20 | **Identifying Semantic Induction Heads to Understand In-Context Learning** | ![](https://img.shields.io/badge/Induction%20Head-blue)  | [![Paper](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2402.13055v1) |
| 2024-02-16 | **The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains** | ![](https://img.shields.io/badge/Induction%20Head-blue) | [![Paper](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2402.11004) |
| 2024-02-11 | **Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs** | ![](https://img.shields.io/badge/Mover%20Head-blue) ![](https://img.shields.io/badge/Relation%20Head-blue) | [![Paper](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.arxiv.org/abs/2402.07321) |
| 2024-02-05 | **How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning** | ![](https://img.shields.io/badge/In--Context%20Head-blue) | [![Paper](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2402.02872) |
| 2024-01-16 | **Function Vectors in Large Language Models** | ![](https://img.shields.io/badge/Function%20Vector%20Head-blue) | [![Paper](https://img.shields.io/badge/ICLR-Paper-%23D2691E)](https://openreview.net/forum?id=AwyxtyMwaG&noteId=6Qv7kx00La) [![Project](https://img.shields.io/badge/Git-Page-black?logo=internet-explorer)](https://functions.baulab.info/) [![Code](https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github)](https://github.com/ericwtodd/function_vectors) [![Data](https://img.shields.io/badge/GitHub-Data-brightgreen?logo=github)](https://github.com/ericwtodd/function_vectors/tree/main/dataset_files) |
| 2023-10-06 | **Copy Suppression: Comprehensively Understanding an Attention Head** | ![](https://img.shields.io/badge/Copy%20Suppression%20Head-blue) | [![Paper](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2310.04625) [![Demo](https://img.shields.io/badge/Demo-View-purple?logo=internet-explorer)](https://copy-suppression.streamlit.app/) |
| 2023-09-22 | **Inference-Time Intervention: Eliciting Truthful Answers from a Language Model** | ![](https://img.shields.io/badge/Truthfulness%20Head-blue) | [![Paper](https://img.shields.io/badge/NeurIPS-Paper-%23D2691E)](https://openreview.net/forum?id=aLLuYpn83y) [![Code](https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github)](https://github.com/likenneth/honest_llama) |
| 2023-09-22 | **Birth of a Transformer: A Memory Viewpoint** | ![](https://img.shields.io/badge/Induction%20Head-blue) | [![Paper](https://img.shields.io/badge/NeurIPS-Paper-%23D2691E)](https://openreview.net/forum?id=3X2EbBLNsk) [![Code](https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github)](https://github.com/albietz/transformer-birth) |
| 2022-10-01 | **Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small** | ![](https://img.shields.io/badge/Duplicate%20Token%20Head-blue) ![](https://img.shields.io/badge/Induction%20Head-blue) ![](https://img.shields.io/badge/S--Inhibition%20Head-blue) | [![Paper](https://img.shields.io/badge/ICLR-Paper-%23D2691E)](https://openreview.net/forum?id=NpsVSN6o4ul) [![Code](https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github)](https://github.com/redwoodresearch/Easy-Transformer) |
| 2022-03-08 | **In-context Learning and Induction Heads** | ![](https://img.shields.io/badge/Induction%20Head-blue) | [![Paper](https://img.shields.io/badge/Anthropic-Paper-%23D2691E)](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) |
| 2021-12-22 | **A Mathematical Framework for Transformer Circuits** | ![](https://img.shields.io/badge/Induction%20Head-blue) | [![Paper](https://img.shields.io/badge/Anthropic-Paper-%23D2691E)](https://transformer-circuits.pub/2021/framework/index.html) |
| 2019-09-06 | **Adaptively Sparse Transformers** | ![](https://img.shields.io/badge/Positional%20Head-blue) ![](https://img.shields.io/badge/BPE--merging%20Head-blue) ![](https://img.shields.io/badge/Interrogation%20Head-blue) | [![Paper](https://img.shields.io/badge/EMNLP-Paper-%23D2691E)](https://aclanthology.org/D19-1223/) [![Code](https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github)](https://github.com/deep-spin/entmax) |
| 2019-05-22 | **Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned** | ![](https://img.shields.io/badge/Positional%20Head-blue) ![](https://img.shields.io/badge/Syntactic%20Head-blue) ![](https://img.shields.io/badge/Rare%20words%20Head-blue) | [![Paper](https://img.shields.io/badge/ACL-Paper-%23D2691E)](https://aclanthology.org/P19-1580/) [![Code](https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github)](https://github.com/lena-voita/the-story-of-heads) |
| 2016-03-21 | **Incorporating Copying Mechanism in Sequence-to-Sequence Learning** | ![](https://img.shields.io/badge/Retrieval%20Head-blue)  | [![Paper](https://img.shields.io/badge/ACL-Paper-%23D2691E)](https://aclanthology.org/P16-1154/) |
