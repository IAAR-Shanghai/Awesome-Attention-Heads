# Awesome-Attention-Heads

[![Awesome Attention Heads](https://img.shields.io/static/v1?label=&message=Awesome+Attention+Heads&color=black&logo=awesomelists)](https://github.com/IAAR-Shanghai/Awesome-Attention-Heads)
![](https://img.shields.io/github/last-commit/IAAR-Shanghai/Awesome-Attention-Heads?color=green)

With the development of large language models, their underlying network structure, the Transformer, is being extensively studied. Researching the Transformer structure helps us enhance our understanding of this "black box" and improve model interpretability. Recently, there has been an increasing body of work suggesting that the model contains two distinct partitions: attention mechanisms used for behavior, inference, and analysis, and feed-forward networks (FFN) for knowledge storage. The former is crucial for revealing the functional capabilities of the model, leading to a series of studies exploring various functions within attention mechanisms, which we have termed **Attention Head Mining**.

Ordered by publication date:

* ![](https://img.shields.io/badge/Iteration%20Head-blue) Iteration Head: A Mechanistic Study of Chain-of-Thought. arXiv 2024. [[Paper](https://arxiv.org/abs/2406.02128)]

* ![](https://img.shields.io/badge/Induction%20Head-blue) Induction Heads as a Primary Mechanism for Pattern Matching in In-context Learning. OpenReview 2024. [[Paper](https://openreview.net/forum?id=np6hrTv7aW)]

* ![](https://img.shields.io/badge/Retrieval%20Head-blue) Retrieval Head Mechanistically Explains Long-Context Factuality. arXiv 2024. [[Paper](https://arxiv.org/abs/2404.15574)] [[Code](https://github.com/nightdessert/Retrieval_Head)]

* ![](https://img.shields.io/badge/Truthfulness%20Head-blue) Non-Linear Inference Time Intervention: Improving LLM Truthfulness. arXiv 2024. [[Paper](https://arxiv.org/abs/2403.18680)] [[Code](https://github.com/Samsung/NL-ITI)]

* ![](https://img.shields.io/badge/Induction%20Head-blue) Identifying Semantic Induction Heads to Understand In-Context Learning. arXiv 2024. [[Paper](https://arxiv.org/abs/2402.13055v1)]

* ![](https://img.shields.io/badge/Induction%20Head-blue) The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains. arXiv 2024. [[Paper](https://arxiv.org/abs/2402.11004)]

* ![](https://img.shields.io/badge/In--Context%20Head-blue) How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning. arXiv 2024. [[Paper](https://arxiv.org/abs/2402.02872)]

* ![](https://img.shields.io/badge/Function%20Vector%20Head-blue) Function Vectors in Large Language Models. ICLR 2024. [[Paper](https://openreview.net/forum?id=AwyxtyMwaG&noteId=6Qv7kx00La)] [[Project](https://functions.baulab.info/)] [[Code](https://github.com/ericwtodd/function_vectors)] [[Data](https://github.com/ericwtodd/function_vectors/tree/main/dataset_files)]

* ![](https://img.shields.io/badge/Truthfulness%20Head-blue) Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. NeurIPS 2023. [[Paper](https://openreview.net/forum?id=aLLuYpn83y)] [[Code](https://github.com/likenneth/honest_llama)]

* ![](https://img.shields.io/badge/Induction%20Head-blue) Birth of a Transformer: A Memory Viewpoint. NeurIPS 2023. [[Paper](https://openreview.net/forum?id=3X2EbBLNsk)] [[Code](https://github.com/albietz/transformer-birth)]

* ![](https://img.shields.io/badge/Copy%20Suppression%20Head-blue) Copy Suppression: Comprehensively Understanding an Attention Head. arXiv 2023. [[Paper](https://arxiv.org/abs/2310.04625)] [[Demo](https://copy-suppression.streamlit.app/)]

* ![](https://img.shields.io/badge/Induction%20Head-blue) In-context Learning and Induction Heads. Transformer Circuits Thread 2022. [[Paper](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)]

* ![](https://img.shields.io/badge/Induction%20Head-blue) A Mathematical Framework for Transformer Circuits. Transformer Circuits Thread 2021. [[Paper](https://transformer-circuits.pub/2021/framework/index.html)]

* ![](https://img.shields.io/badge/Positional%20Head-blue) ![](https://img.shields.io/badge/Syntactic%20Head-blue) ![](https://img.shields.io/badge/Rare%20words%20Head-blue) Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned. ACL 2019. [[Paper](https://aclanthology.org/P19-1580/)] [[Code](https://github.com/lena-voita/the-story-of-heads)]

* ![](https://img.shields.io/badge/Retrieval%20Head-blue) Incorporating Copying Mechanism in Sequence-to-Sequence Learning. ACL 2016. [[Paper](https://aclanthology.org/P16-1154/)]
