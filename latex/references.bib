%%%%
% Survey
%%%%
@inproceedings{SurveyDNNInner_23_SaTML_MIT,
  title={Toward transparent ai: A survey on interpreting the inner structures of deep neural networks},
  author={R{\"a}uker, Tilman and Ho, Anson and Casper, Stephen and Hadfield-Menell, Dylan},
  booktitle={2023 ieee conference on secure and trustworthy machine learning (satml)},
  pages={464--483},
  year={2023},
  organization={IEEE}
}
@article{SurveyNeurAttn_21_arXiv_Brazil,
  title={Neural attention models in deep learning: Survey and taxonomy},
  author={Santana, Alana and Colombini, Esther},
  journal={arXiv preprint arXiv:2112.05909},
  year={2021}
}
@article{SurveyMdedical_22_IEEE_Portugal,
  title={A survey on attention mechanisms for medical applications: are we moving toward better Algorithms?},
  author={Gon{\c{c}}alves, Tiago and Rio-Torto, Isabel and Teixeira, Lu{\'\i}s F and Cardoso, Jaime S},
  journal={IEEE Access},
  volume={10},
  pages={98909--98935},
  year={2022},
  publisher={IEEE}
}
@article{SurveyAttentionModel_21_Linkedin,
  title={An attentive survey of attention models},
  author={Chaudhari, Sneha and Mithal, Varun and Polatkan, Gungor and Ramanath, Rohan},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={12},
  number={5},
  pages={1--32},
  year={2021},
  publisher={ACM New York, NY}
}
@inproceedings{SurveyNLPAttn_18_arXiv_GIT,
  title={An introductory survey on attention mechanisms in NLP problems},
  author={Hu, Dichao},
  booktitle={Intelligent Systems and Applications: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys) Volume 2},
  pages={432--448},
  year={2020},
  organization={Springer}
}
@article{SurveydDLAttn_22_arixv_Netherland,
  title={A general survey on attention mechanisms in deep learning},
  author={Brauwers, Gianni and Frasincar, Flavius},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={35},
  number={4},
  pages={3279--3298},
  year={2021},
  publisher={IEEE}
}
@article{SurveyLLMInterp_24_arXiv,
  title={From understanding to utilization: A survey on explainability for large language models},
  author={Luo, Haoyan and Specia, Lucia},
  journal={arXiv preprint arXiv:2401.12874},
  year={2024}
}
@inproceedings{SurveyNLPExplain_20_ACL_IBM,
    title = "A Survey of the State of Explainable {AI} for Natural Language Processing",
    author = "Danilevsky, Marina  and
      Qian, Kun  and
      Aharonov, Ranit  and
      Katsis, Yannis  and
      Kawas, Ban  and
      Sen, Prithviraj",
    editor = "Wong, Kam-Fai  and
      Knight, Kevin  and
      Wu, Hua",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    pages = "447--459",
}
@article{SurveyCTG_24_arXiv_RUC,
  title={Controllable Text Generation for Large Language Models: A Survey},
  author={Liang, Xun and Wang, Hanyu and Wang, Yezhaohui and Song, Shichao and Yang, Jiawei and Niu, Simin and Hu, Jie and Liu, Dan and Yao, Shunyu and Xiong, Feiyu and others},
  journal={arXiv preprint arXiv:2408.12599},
  year={2024}
}



%%%%
% Attention Head
%%%%
@article{InductionHeads_22_TCT_Anthropic,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}
@article{MathFrame_21_TCT_Anthropic,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}
@article{CorrectLetterHead_23_arXiv_DeepMind,
  title={Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla},
  author={Lieberum, Tom and Rahtz, Matthew and Kram{\'a}r, J{\'a}nos and Irving, Geoffrey and Shah, Rohin and Mikulik, Vladimir},
  journal={arXiv preprint arXiv:2307.09458},
  year={2023}
}
@article{CopySupression_23_arXiv_Google,
  title={Copy suppression: Comprehensively understanding an attention head},
  author={McDougall, Callum and Conmy, Arthur and Rushing, Cody and McGrath, Thomas and Nanda, Neel},
  journal={arXiv preprint arXiv:2310.04625},
  year={2023}
}
@article{IOI_23_ICLR_Redwood,
  title={Interpretability in the wild: a circuit for indirect object identification in gpt-2 small},
  author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2211.00593},
  year={2022}
}
@article{SpecialHead_19_ACL_Russia,
  title={Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1905.09418},
  year={2019}
}
@article{Sentiment_23_arXiv_EleutherAI,
  title={Linear representations of sentiment in large language models},
  author={Tigges, Curt and Hollinsworth, Oskar John and Geiger, Atticus and Nanda, Neel},
  journal={arXiv preprint arXiv:2310.15154},
  year={2023}
}
@article{FactualRecall_24_arXiv_Independent,
  title={Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs},
  author={Chughtai, Bilal and Cooney, Alan and Nanda, Neel},
  journal={arXiv preprint arXiv:2402.07321},
  year={2024}
}
@article{Markov_24_arXiv_Harvard,
  title={The evolution of statistical induction heads: In-context learning markov chains},
  author={Edelman, Benjamin L and Edelman, Ezra and Goel, Surbhi and Malach, Eran and Tsilivis, Nikolaos},
  journal={arXiv preprint arXiv:2402.11004},
  year={2024}
}
@article{Semantic_24_arXiv_SJTU,
  title={Identifying semantic induction heads to understand in-context learning},
  author={Ren, Jie and Guo, Qipeng and Yan, Hang and Liu, Dongrui and Qiu, Xipeng and Lin, Dahua},
  journal={arXiv preprint arXiv:2402.13055},
  year={2024}
}
@article{InformationFlow_24_arXiv_Meta,
  title={Information flow routes: Automatically interpreting language models at scale},
  author={Ferrando, Javier and Voita, Elena},
  journal={arXiv preprint arXiv:2403.00824},
  year={2024}
}
@article{KnowledgeConflict_24_arXiv_UCAS,
  title={Cutting off the head ends the conflict: A mechanism for interpreting and mitigating knowledge conflicts in language models},
  author={Jin, Zhuoran and Cao, Pengfei and Yuan, Hongbang and Chen, Yubo and Xu, Jiexin and Li, Huaijun and Jiang, Xiaojian and Liu, Kang and Zhao, Jun},
  journal={arXiv preprint arXiv:2402.18154},
  year={2024}
}
@article{NL-ITI_24_arXiv-Samsung,
  title={NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method},
  author={Hoscilowicz, Jakub and Wiacek, Adam and Chojnacki, Jan and Cieslak, Adam and Michon, Leszek and Urbanevych, Vitalii and Janicki, Artur},
  journal={arXiv preprint arXiv:2403.18680},
  year={2024}
}
@article{RetrievalHead_24_arXiv_PKU,
  title={Retrieval head mechanistically explains long-context factuality},
  author={Wu, Wenhao and Wang, Yizhong and Xiao, Guangxuan and Peng, Hao and Fu, Yao},
  journal={arXiv preprint arXiv:2404.15574},
  year={2024}
}
@article{FSL_24_ICML_UCL,
  title={What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation},
  author={Singh, Aaditya K and Moskovitz, Ted and Hill, Felix and Chan, Stephanie CY and Saxe, Andrew M},
  journal={arXiv preprint arXiv:2404.07129},
  year={2024}
}
@article{HumanMemory_24_arXiv_UCSD,
  title={Linking In-context Learning in Transformers to Human Episodic Memory},
  author={Ji-An, Li and Zhou, Corey Y and Benna, Marcus K and Mattar, Marcelo G},
  journal={arXiv preprint arXiv:2405.14992},
  year={2024}
}
@article{KnowledgeCircuit_24_arXiv_ZJU,
  title={Knowledge Circuits in Pretrained Transformers},
  author={Yao, Yunzhi and Zhang, Ningyu and Xi, Zekun and Wang, Mengru and Xu, Ziwen and Deng, Shumin and Chen, Huajun},
  journal={arXiv preprint arXiv:2405.17969},
  year={2024}
}
@article{IterationHead_24_arXiv_Meta,
  title={Iteration Head: A Mechanistic Study of Chain-of-Thought},
  author={Cabannes, Vivien and Arnal, Charles and Bouaziz, Wassim and Yang, Alice and Charton, Francois and Kempe, Julia},
  journal={arXiv preprint arXiv:2406.02128},
  year={2024}
}
@inproceedings{FaithfulCoT_24_ICML_Harvard,
  title={On the Difficulty of Faithful Chain-of-Thought Reasoning in Large Language Models},
  year={2024},
  author={Tanneru, Sree Harsha and Ley, Dan and Agarwal, Chirag and Lakkaraju, Himabindu},
  booktitle={Trustworthy Multi-modal Foundation Models and AI Agents (TiFA)}
}
@inproceedings{CrossLingual_24_SIGIR_UCAS,
  title={Steering Large Language Models for Cross-lingual Information Retrieval},
  author={Guo, Ping and Ren, Yubing and Hu, Yue and Cao, Yanan and Li, Yunpeng and Huang, Heyan},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={585--596},
  year={2024}
}
@article{CorrectHead_24_arXiv_Allen,
  title={Answer, Assemble, Ace: Understanding How Transformers Answer Multiple Choice Questions},
  author={Wiegreffe, Sarah and Tafjord, Oyvind and Belinkov, Yonatan and Hajishirzi, Hannaneh and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2407.15018},
  year={2024}
}
@article{NegativeHead_24_arXiv_SNU,
  title={Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment},
  author={Yu, Sangwon and Song, Jongyoon and Hwang, Bongkyu and Kang, Hoyoung and Cho, Sooah and Choi, Junhwa and Joe, Seongho and Lee, Taehee and Gwon, Youngjune L and Yoon, Sungroh},
  journal={arXiv preprint arXiv:2408.00137},
  year={2024}
}
@article{VulnerableHead_24_arXiv_Alicante,
  title={Detecting and Understanding Vulnerabilities in Language Models via Mechanistic Interpretability},
  author={Garc{\'\i}a-Carrasco, Jorge and Mat{\'e}, Alejandro and Trujillo, Juan},
  journal={arXiv preprint arXiv:2407.19842},
  year={2024}
}
@inproceedings{AcronymPredict_24_arXiv_Alicante,
  title={How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via Mechanistic Interpretability},
  author={Garc{\'\i}a-Carrasco, Jorge and Mat{\'e}, Alejandro and Trujillo, Juan Carlos},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3322--3330},
  year={2024},
  organization={PMLR}
}
@article{GlobalAttention_24_arXiv_THU,
  title={MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression},
  author={Fu, Tianyu and Huang, Haofeng and Ning, Xuefei and Zhang, Genghan and Chen, Boju and Wu, Tianqi and Wang, Hongyi and Huang, Zixiao and Li, Shiyao and Yan, Shengen and others},
  journal={arXiv preprint arXiv:2406.14909},
  year={2024}
}
@article{RetrievalHead_24_arXiv_Huawei,
  title={RazorAttention: Efficient KV Cache Compression Through Retrieval Heads},
  author={Tang, Hanlin and Lin, Yang and Lin, Jing and Han, Qingsen and Hong, Shikuan and Yao, Yiwu and Wang, Gongyi},
  journal={arXiv preprint arXiv:2407.15891},
  year={2024}
}



%%%%
% 与Attention相关但没有提出新的head
%%%%
@article{MaskLayer_24_arXiv_NUDT,
  title={Look Within, Why LLMs Hallucinate: A Causal Perspective},
  author={Li, He and Chi, Haoang and Liu, Mingyu and Yang, Wenjing},
  journal={arXiv preprint arXiv:2407.10153},
  year={2024}
}
@article{AttnLookback_24_arXiv_MIT,
  title={Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps},
  author={Chuang, Yung-Sung and Qiu, Linlu and Hsieh, Cheng-Yu and Krishna, Ranjay and Kim, Yoon and Glass, James},
  journal={arXiv preprint arXiv:2407.07071},
  year={2024}
}
@article{ITI_23_NIPS_harvard,
  title={Inference-time intervention: Eliciting truthful answers from a language model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{FunctionVector_24_ICLR_NEU,
  title={Function vectors in large language models},
  author={Todd, Eric and Li, Millicent L and Sharma, Arnab Sen and Mueller, Aaron and Wallace, Byron C and Bau, David},
  journal={arXiv preprint arXiv:2310.15213},
  year={2023}
}
@article{WordClassification_24_arXiv_UoM,
  title={How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning},
  author={Yu, Zeping and Ananiadou, Sophia},
  journal={arXiv preprint arXiv:2402.02872},
  year={2024}
}
@article{InductionHead_24_arXiv_UoA,
  title={Induction Heads as an Essential Mechanism for Pattern Matching in In-context Learning},
  author={Crosbie, Joy},
  journal={arXiv preprint arXiv:2407.07011},
  year={2024}
}
@inproceedings{ColorObject_24_ICLR_BrownU,
  title={Circuit Component Reuse Across Tasks in Transformer Language Models},
  author={Merullo, Jack and Eickhoff, Carsten and Pavlick, Ellie},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@article{SuccessorHead_24_ICLR_Cambridge,
  title={Successor heads: Recurring, interpretable attention heads in the wild},
  author={Gould, Rhys and Ong, Euan and Ogden, George and Conmy, Arthur},
  journal={arXiv preprint arXiv:2312.09230},
  year={2023}
}
@article{AssociativeMemory_23_NIPS_Meta,
  title={Birth of a transformer: A memory viewpoint},
  author={Bietti, Alberto and Cabannes, Vivien and Bouchacourt, Diane and Jegou, Herve and Bottou, Leon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}





%%%%
% 探究技术&方法
%%%%
@article{ActivationPatching_22_NIPS_MIT,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}
@article{InterpTermExplain_Blog,
  title={A Comprehensive Mechanistic Interpretability Explainer \& Glossary},
  author={Nanda, Neel},
  journal={Neel Nanda’s Blog},
  volume={1},
  year={2022}
}
@article{OldCircuit_20_distill_OpenAI,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}
@article{ActivationAddition,
  title={Activation addition: Steering language models without optimization},
  author={Turner, Alexander Matt and Thiergart, Lisa and Leech, Gavin and Udell, David and Vazquez, Juan J and Mini, Ulisse and MacDiarmid, Monte},
  journal={arXiv preprint arXiv:2308.10248},
  year={2023}
}
@inproceedings{ActivationPatching_24_ICLR_Google,
    title={Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
    author={Fred Zhang and Neel Nanda},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
}
@article{ActivationPatching_24_arXiv_Google,
  title={How to use and interpret activation patching},
  author={Heimersheim, Stefan and Nanda, Neel},
  journal={arXiv preprint arXiv:2404.15255},
  year={2024}
}
@article{ActivationAddition_23_arXiv_DeepMind,
  title={Activation addition: Steering language models without optimization},
  author={Turner, Alexander Matt and Thiergart, Lisa and Leech, Gavin and Udell, David and Vazquez, Juan J and Mini, Ulisse and MacDiarmid, Monte},
  journal={arXiv preprint arXiv:2308.10248},
  year={2023}
}
@inproceedings{SemanticConsistency_24_ACL_TJU,
    title = "Enhancing Semantic Consistency of Large Language Models through Model Editing: An Interpretability-Oriented Approach",
    author = "Yang, Jingyuan  and
      Chen, Dapeng  and
      Sun, Yajing  and
      Li, Rongjun  and
      Feng, Zhiyong  and
      Peng, Wei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    pages = "3343--3353",
}
@misc{LogitLens_colab,
    title  = {Logit {Lens} on Non-{GPT2} Models + Extensions},
    author = {nostalgebraist},
    url    = {https://colab.research.google.com/drive/1MjdfK2srcerLrAJDRaJQKO0sUiZ-hQtA},
    year   = {2021}
}



%%%
% 认知神经学、心理学相关
%%%
@article{AIHumanGAP_24_Intelligence_UWA,
  title={Defining intelligence: Bridging the gap between human and artificial perspectives},
  author={Gignac, Gilles E and Szodorai, Eva T},
  journal={Intelligence},
  volume={104},
  pages={101832},
  year={2024},
  publisher={Elsevier}
}
@article{CoT_22_NIPS_Google,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{MemoryRecall,
  title={“Episodic and semantic memory,” in Organization of Memory},
  author={Tulving, Endel},
  journal={(No Title)},
  pages={381},
  year={1972}
}
@article{SquireMemory,
  title={Memory and the hippocampus: a synthesis from findings with rats, monkeys, and humans.},
  author={Squire, Larry R},
  journal={Psychological review},
  volume={99},
  number={2},
  pages={195},
  year={1992},
  publisher={American Psychological Association}
}
@article{GeneralStructure,
  title={The role of knowledge in discourse comprehension: a construction-integration model.},
  author={Kintsch, Walter},
  journal={Psychological review},
  volume={95},
  number={2},
  pages={163},
  year={1988},
  publisher={American Psychological Association}
}
@book{Syntax,
  title={Aspects of the Theory of Syntax},
  author={Chomsky, Noam},
  number={11},
  year={2014},
  publisher={MIT press}
}
@book{Semantic,
  title={Semantic structures},
  author={Jackendoff, Ray S},
  volume={18},
  year={1992},
  publisher={MIT press}
}
@book{ReasoningNumber,
  title={The number sense: How the mind creates mathematics},
  author={Dehaene, Stanislas},
  year={2011},
  publisher={OUP USA}
}
@book{ReasoningLogic,
  title={Mental models. Towards a cognitive science of language, inference, and consciousness},
  author={Johnson-Laird, PN},
  year={1983},
  publisher={Harvard University Press}
}
@article{LanguageExpress,
  title={Models of word production},
  author={Levelt, Willem JM},
  journal={Trends in cognitive sciences},
  volume={3},
  number={6},
  pages={223--232},
  year={1999},
  publisher={Elsevier}
}
@article{MachinePsychology_Hagendorff,
  title={Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods},
  author={Hagendorff, Thilo},
  journal={arXiv preprint arXiv:2303.13988},
  year={2023}
}
@article{MachinePsychology_Johansson,
  title={Functional Equivalence with NARS},
  author={Johansson, Robert and Hammer, Patrick and Lofthouse, Tony},
  journal={arXiv preprint arXiv:2405.03340},
  year={2024}
}
@article{OperantConditioning,
  title={Operant conditioning},
  author={Staddon, John ER and Cerutti, Daniel T},
  journal={Annual review of psychology},
  volume={54},
  number={1},
  pages={115--144},
  year={2003},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}
@article{MachinePsychologyOrigin,
  title={Machine psychology: autonomous behavior, perceptual categorization and conditioning in a brain-based device},
  author={Krichmar, Jeffrey L and Edelman, Gerald M},
  journal={Cerebral Cortex},
  volume={12},
  number={8},
  pages={818--830},
  year={2002},
  publisher={Oxford University Press}
}



%%%%
% Other Task
%%%%
@article{FacutalRecall_23_EMNLP_Google,
  title={Dissecting recall of factual associations in auto-regressive language models},
  author={Geva, Mor and Bastings, Jasmijn and Filippova, Katja and Globerson, Amir},
  journal={arXiv preprint arXiv:2304.14767},
  year={2023}
}
@article{FactualRecall_24_arXiv_RUC,
  title={Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models},
  author={Lv, Ang and Zhang, Kaiyi and Chen, Yuhan and Wang, Yulong and Liu, Lifeng and Wen, Ji-Rong and Xie, Jian and Yan, Rui},
  journal={arXiv preprint arXiv:2403.19521},
  year={2024}
}
@article{MLPKnowledge,
  title={Knowledge neurons in pretrained transformers},
  author={Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  journal={arXiv preprint arXiv:2104.08696},
  year={2021}
}
@article{TransformerKVpair,
  title={Transformer feed-forward layers are key-value memories},
  author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  journal={arXiv preprint arXiv:2012.14913},
  year={2020}
}
@article{Memory3_24_arXiv_IAAR,
  title={$$\backslash$text $\{$Memory$\}$\^{} 3$: Language Modeling with Explicit Memory},
  author={Yang, Hongkang and Lin, Zehao and Wang, Wenjin and Wu, Hao and Li, Zhiyu and Tang, Bo and Wei, Wenqiang and Wang, Jinbo and Tang, Zeyun and Song, Shichao and others},
  journal={arXiv preprint arXiv:2407.01178},
  year={2024}
}
@inproceedings{MLPLocalUpdate_23_EMNLP_ETH,
    title = "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
    author = "Stolfo, Alessandro  and
      Belinkov, Yonatan  and
      Sachan, Mrinmaya",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    pages = "7035--7052",
}


%%
% Evaluation
%%
@inproceedings{SST_Stanford,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}
@article{MMLU_UCB,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}
@article{TriviaQA_UoW,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}
@article{TruthfulQA_Oxford,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}
@article{AGNews_NYU,
  title={Character-level convolutional networks for text classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@article{AGENDA_UoW,
  title={Text generation from knowledge graphs with graph transformers},
  author={Koncel-Kedziorski, Rik and Bekal, Dhanush and Luan, Yi and Lapata, Mirella and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1904.02342},
  year={2019}
}
@misc{Needle_Github,
  title={LLM Test: Needle-In-A-Haystack},
  author={Kedar, Chandrayan and Lance, Martin and Lazaro, Hurtado and Prabha, Arivalagan and Pavel, Kral and Eltociear Ashimine},
  year={2023},
  publisher={GitHub},
  journal={GitHub repository},
  howpublished = {\url{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}},
}
@article{LRE_MIT,
  title={Linearity of relation decoding in transformer language models},
  author={Hernandez, Evan and Sharma, Arnab Sen and Haklay, Tal and Meng, Kevin and Wattenberg, Martin and Andreas, Jacob and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2308.09124},
  year={2023}
}


%%%%
% 只是引用一下
%%%%
@article{AttentionIsAllYouNeed,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{vig2019analyzing,
  title={Analyzing the structure of attention in a transformer language model},
  author={Vig, Jesse and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:1906.04284},
  year={2019}
}
@article{LLMblackbox_lipton2018,
  title={The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery.},
  author={Lipton, Zachary C},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018},
  publisher={ACM New York, NY, USA}
}
@inproceedings{LLMblackbox_gilpin2018,
  title={Explaining explanations: An overview of interpretability of machine learning},
  author={Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  booktitle={2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)},
  pages={80--89},
  year={2018},
  organization={IEEE}
}
@article{BERT_MODEL,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{GPT2_MODEL,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{DNN_Interp_Montavon,
  title={Methods for interpreting and understanding deep neural networks},
  author={Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Digital signal processing},
  volume={73},
  pages={1--15},
  year={2018},
  publisher={Elsevier}
}
@inproceedings{liu-etal-2020-understanding,
    title = "Understanding the Difficulty of Training Transformers",
    author = "Liu, Liyuan  and
      Liu, Xiaodong  and
      Gao, Jianfeng  and
      Chen, Weizhu  and
      Han, Jiawei",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "5747--5763",
}
@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}
@article{ScalingLaw_20_arXiv_OpenAI,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
@article{ScalingLaw_21_arXiv_Stanford,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
@article{ICSF_24_arXiv_IAAR,
  title={Internal Consistency and Self-Feedback in Large Language Models: A Survey},
  author={Liang, Xun and Song, Shichao and Zheng, Zifan and Wang, Hanyu and Yu, Qingchen and Li, Xunkai and Li, Rong-Hua and Xiong, Feiyu and Li, Zhiyu},
  journal={arXiv preprint arXiv:2407.14507},
  year={2024}
}
@article{GLU_2020_arXiv_Google,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}
@article{ROPE_24_Neuro_Zhuiyi,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}
@inproceedings{FewShotLearners,
    title = "Language Models are Few-shot Multilingual Learners",
    author = "Winata, Genta Indra  and
      Madotto, Andrea  and
      Lin, Zhaojiang  and
      Liu, Rosanne  and
      Yosinski, Jason  and
      Fung, Pascale",
    editor = "Ataman, Duygu  and
      Birch, Alexandra  and
      Conneau, Alexis  and
      Firat, Orhan  and
      Ruder, Sebastian  and
      Sahin, Gozde Gul",
    booktitle = "Proceedings of the 1st Workshop on Multilingual Representation Learning",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    pages = "1--15",
}
@article{MOETransformer,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}
@mastersthesis{ICLDisentagle_23_thesis,
  title={What in-context learning “learns” in-context: Disentangling task recognition and task learning},
  author={Pan, Jane},
  year={2023},
  school={Princeton University}
}
@article{xFinder_24_arXiv_IAAR,
  title={xFinder: Robust and Pinpoint Answer Extraction for Large Language Models},
  author={Yu, Qingchen and Zheng, Zifan and Song, Shichao and Li, Zhiyu and Xiong, Feiyu and Tang, Bo and Chen, Ding},
  journal={arXiv preprint arXiv:2405.11874},
  year={2024}
}