%%%%
% Attention Head
%%%%
@article{InductionHeads_22_TCT_Anthropic,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={Preprint at arXiv},
  year={2022},
  note={\href{https://doi.org/10.48550/arXiv.2209.11895}{https://doi.org/10.48550/arXiv.2209.11895}},
}
@article{MathFrame_21_TCT_Anthropic,
  title={A mathematical framework for transformer circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and others},
  journal={Transformer Circuits Thread},
  volume={1},
  number={1},
  pages={12},
  year={2021}
}
@article{CorrectLetterHead_23_arXiv_DeepMind,
  title={Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla},
  author={Lieberum, Tom and Rahtz, Matthew and Kram{\'a}r, J{\'a}nos and Irving, Geoffrey and Shah, Rohin and Mikulik, Vladimir},
  journal={Preprint at arXiv},
  year={2023},
  note={\href{https://doi.org/10.48550/arXiv.2307.09458}{https://doi.org/10.48550/arXiv.2307.09458}},
}
@article{CopySupression_23_arXiv_Google,
  title={Copy suppression: Comprehensively understanding an attention head},
  author={McDougall, Callum and Conmy, Arthur and Rushing, Cody and McGrath, Thomas and Nanda, Neel},
  journal={Preprint at arXiv},
  year={2023},
  note={\href{https://doi.org/10.48550/arXiv.2310.04625}{https://doi.org/10.48550/arXiv.2310.04625}},
}
@inproceedings{IOI_23_ICLR_Redwood,
    title={Interpretability in the Wild: a Circuit for Indirect Object Identification in {GPT}-2 Small},
    author={Kevin Ro Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
    booktitle={The Eleventh International Conference on Learning Representations},
    year={2023},
    pages = {1--21},
}
@inproceedings{SpecialHead_19_ACL_Russia,
    title = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
    author = {Voita, Elena  and Talbot, David  and Moiseev, Fedor  and Sennrich, Rico  and Titov, Ivan},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year = {2019},
    publisher = {Association for Computational Linguistics},
    pages = {5797--5808},
}
@article{Sentiment_23_arXiv_EleutherAI,
  title={Linear representations of sentiment in large language models},
  author={Tigges, Curt and Hollinsworth, Oskar John and Geiger, Atticus and Nanda, Neel},
  journal={Preprint at arXiv},
  year={2023},
  note={\href{https://doi.org/10.48550/arXiv.2310.15154}{https://doi.org/10.48550/arXiv.2310.15154}},
}
@article{FactualRecall_24_arXiv_Independent,
  title={Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs},
  author={Chughtai, Bilal and Cooney, Alan and Nanda, Neel},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2402.07321}{https://doi.org/10.48550/arXiv.2402.07321}},
}
@article{Markov_24_arXiv_Harvard,
  title={The evolution of statistical induction heads: In-context learning markov chains},
  author={Edelman, Benjamin L and Edelman, Ezra and Goel, Surbhi and Malach, Eran and Tsilivis, Nikolaos},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2402.11004}{https://doi.org/10.48550/arXiv.2402.11004}},
}
@inproceedings{Semantic_24_arXiv_SJTU,
    title = "Identifying Semantic Induction Heads to Understand In-Context Learning",
    author = "Ren, Jie  and Guo, Qipeng  and Yan, Hang  and Liu, Dongrui  and Zhang, Quanshi  and Qiu, Xipeng  and Lin, Dahua",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    year = "2024",
    publisher = "Association for Computational Linguistics",
    pages = "6916--6932",
}
@article{InformationFlow_24_arXiv_Meta,
  title={Information flow routes: Automatically interpreting language models at scale},
  author={Ferrando, Javier and Voita, Elena},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2403.00824}{https://doi.org/10.48550/arXiv.2403.00824}},
}
@inproceedings{KnowledgeConflict_24_arXiv_UCAS,
    title = "Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models",
    author = "Jin, Zhuoran  and Cao, Pengfei  and Yuan, Hongbang  and Chen, Yubo  and Xu, Jiexin  and Li, Huaijun  and Jiang, Xiaojian  and Liu, Kang  and Zhao, Jun",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    year = "2024",
    publisher = "Association for Computational Linguistics",
    pages = "1193--1215",
}
@article{NL-ITI_24_arXiv-Samsung,
  title={NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method},
  author={Hoscilowicz, Jakub and Wiacek, Adam and Chojnacki, Jan and Cieslak, Adam and Michon, Leszek and Urbanevych, Vitalii and Janicki, Artur},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2403.18680}{https://doi.org/10.48550/arXiv.2403.18680}},
}
@article{RetrievalHead_24_arXiv_PKU,
  title={Retrieval head mechanistically explains long-context factuality},
  author={Wu, Wenhao and Wang, Yizhong and Xiao, Guangxuan and Peng, Hao and Fu, Yao},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2404.15574}{https://doi.org/10.48550/arXiv.2404.15574}},
}
@InProceedings{FSL_24_ICML_UCL,
  title = 	 {What needs to go right for an induction head? {A} mechanistic study of in-context learning circuits and their formation},
  author =       {Singh, Aaditya K and Moskovitz, Ted and Hill, Felix and Chan, Stephanie C.Y. and Saxe, Andrew M},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {45637--45662},
  year = 	 {2024},
  volume = 	 {235},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
}

@article{HumanMemory_24_arXiv_UCSD,
  title={Linking In-context Learning in Transformers to Human Episodic Memory},
  author={Ji-An, Li and Zhou, Corey Y and Benna, Marcus K and Mattar, Marcelo G},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2405.14992}{https://doi.org/10.48550/arXiv.2405.14992}},
}
@article{KnowledgeCircuit_24_arXiv_ZJU,
  title={Knowledge Circuits in Pretrained Transformers},
  author={Yao, Yunzhi and Zhang, Ningyu and Xi, Zekun and Wang, Mengru and Xu, Ziwen and Deng, Shumin and Chen, Huajun},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2405.17969}{https://doi.org/10.48550/arXiv.2405.17969}},
}
@article{IterationHead_24_arXiv_Meta,
  title={Iteration Head: A Mechanistic Study of Chain-of-Thought},
  author={Cabannes, Vivien and Arnal, Charles and Bouaziz, Wassim and Yang, Alice and Charton, Francois and Kempe, Julia},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2406.02128}{https://doi.org/10.48550/arXiv.2406.02128}}
}
@inproceedings{FaithfulCoT_24_ICML_Harvard,
  title={On the Difficulty of Faithful Chain-of-Thought Reasoning in Large Language Models},
  year={2024},
  author={Tanneru, Sree Harsha and Ley, Dan and Agarwal, Chirag and Lakkaraju, Himabindu},
  booktitle={Trustworthy Multi-modal Foundation Models and AI Agents (TiFA)},
  pages = {1--16},
}
@inproceedings{CrossLingual_24_SIGIR_UCAS,
  title={Steering Large Language Models for Cross-lingual Information Retrieval},
  author={Guo, Ping and Ren, Yubing and Hu, Yue and Cao, Yanan and Li, Yunpeng and Huang, Heyan},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={585--596},
  year={2024}
}
@article{CorrectHead_24_arXiv_Allen,
  title={Answer, Assemble, Ace: Understanding How Transformers Answer Multiple Choice Questions},
  author={Wiegreffe, Sarah and Tafjord, Oyvind and Belinkov, Yonatan and Hajishirzi, Hannaneh and Sabharwal, Ashish},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2407.15018}{https://doi.org/10.48550/arXiv.2407.15018}},
}
@article{NegativeHead_24_arXiv_SNU,
  title={Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment},
  author={Yu, Sangwon and Song, Jongyoon and Hwang, Bongkyu and Kang, Hoyoung and Cho, Sooah and Choi, Junhwa and Joe, Seongho and Lee, Taehee and Gwon, Youngjune L and Yoon, Sungroh},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2408.00137}{https://doi.org/10.48550/arXiv.2408.00137}},
}
@inproceedings{VulnerableHead_24_arXiv_Alicante,
  title     = {Detecting and Understanding Vulnerabilities in Language Models via Mechanistic Interpretability},
  author    = {García-Carrasco, Jorge and Maté, Alejandro and Trujillo, Juan},
  booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, {IJCAI-24}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {385--393},
  year      = {2024},
}
@inproceedings{AcronymPredict_24_arXiv_Alicante,
  title={How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via Mechanistic Interpretability},
  author={Garc{\'\i}a-Carrasco, Jorge and Mat{\'e}, Alejandro and Trujillo, Juan Carlos},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3322--3330},
  year={2024},
  organization={PMLR}
}
@article{GlobalAttention_24_arXiv_THU,
  title={MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression},
  author={Fu, Tianyu and Huang, Haofeng and Ning, Xuefei and Zhang, Genghan and Chen, Boju and Wu, Tianqi and Wang, Hongyi and Huang, Zixiao and Li, Shiyao and Yan, Shengen and others},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2406.14909}{https://doi.org/10.48550/arXiv.2406.14909}},
}
@article{RetrievalHead_24_arXiv_Huawei,
  title={RazorAttention: Efficient KV Cache Compression Through Retrieval Heads},
  author={Tang, Hanlin and Lin, Yang and Lin, Jing and Han, Qingsen and Hong, Shikuan and Yao, Yiwu and Wang, Gongyi},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2407.15891}{https://doi.org/10.48550/arXiv.2407.15891}},
}
@inproceedings{InductionHead_24_ICLR_Princeton,
    title={The mechanistic basis of data dependence and abrupt learning in an in-context classification task},
    author={Gautam Reddy},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    pages={1--14},
}
@inproceedings{FactualRecall_23_EMNLP_DeepMind,
    title = "Dissecting Recall of Factual Associations in Auto-Regressive Language Models",
    author = "Geva, Mor and Bastings, Jasmijn and Filippova, Katja and Globerson, Amir",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",
    publisher = "Association for Computational Linguistics",
    pages = "12216--12235",
}
@article{MaskLayer_24_arXiv_NUDT,
  title={Look Within, Why LLMs Hallucinate: A Causal Perspective},
  author={Li, He and Chi, Haoang and Liu, Mingyu and Yang, Wenjing},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2407.10153}{https://doi.org/10.48550/arXiv.2407.10153}},
}
@article{AttnLookback_24_arXiv_MIT,
  title={Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps},
  author={Chuang, Yung-Sung and Qiu, Linlu and Hsieh, Cheng-Yu and Krishna, Ranjay and Kim, Yoon and Glass, James},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2407.07071}{https://doi.org/10.48550/arXiv.2407.07071}},
}
@article{ITI_23_NIPS_harvard,
  title={Inference-time intervention: Eliciting truthful answers from a language model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024},
  pages={41451--41530},
}
@inproceedings{FunctionVector_24_ICLR_NEU,
    title={Function Vectors in Large Language Models},
    author={Eric Todd and Millicent Li and Arnab Sen Sharma and Aaron Mueller and Byron C Wallace and David Bau},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    pages={1--52},
}
@article{WordClassification_24_arXiv_UoM,
  title={How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning},
  author={Yu, Zeping and Ananiadou, Sophia},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2402.02872}{https://doi.org/10.48550/arXiv.2402.02872}},
}
@article{InductionHead_24_arXiv_UoA,
  title={Induction Heads as an Essential Mechanism for Pattern Matching in In-context Learning},
  author={Crosbie, Joy},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2407.07011}{https://doi.org/10.48550/arXiv.2407.07011}},
}
@inproceedings{ColorObject_24_ICLR_BrownU,
  title={Circuit Component Reuse Across Tasks in Transformer Language Models},
  author={Merullo, Jack and Eickhoff, Carsten and Pavlick, Ellie},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  pages={1--29},
}
@inproceedings{SuccessorHead_24_ICLR_Cambridge,
    title={Successor Heads: Recurring, Interpretable Attention Heads In The Wild},
    author={Rhys Gould and Euan Ong and George Ogden and Arthur Conmy},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    pages={1--26},
}
@article{AssociativeMemory_23_NIPS_Meta,
  title={Birth of a transformer: A memory viewpoint},
  author={Bietti, Alberto and Cabannes, Vivien and Bouchacourt, Diane and Jegou, Herve and Bottou, Leon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024},
  pages={1560--1588},
}
@inproceedings{PositionalHead_18_ACL_Helsinki,
    title = "An Analysis of Encoder Representations in Transformer-Based Machine Translation",
    author = {Raganato, Alessandro and Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    year = "2018",
    publisher = "Association for Computational Linguistics",
    pages = "287--297",
}
@article{PreviousHead_23_AIForum_Google,  
    title={Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level},
    journal={Alignment Forum},  
    author={Nanda, Neel and Rajamanoharan, Senthooran and Kram{\'a}r, J{\'a}nos and Shah, Rohin},
    pages={19},
    year={2023}
} 
@inproceedings{SubwordHead_19_ACL_Portugal,
    title = "Adaptively Sparse Transformers",
    author = "Correia, Gon{\c{c}}alo M. and Niculae, Vlad  and Martins, Andr{\'e} F. T.",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    pages = "2174--2184",
}
@inproceedings{SyntacticHead_23_arXiv_NYU,
    title={Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in {MLM}s},
    author={Angelica Chen and Ravid Shwartz-Ziv and Kyunghyun Cho and Matthew L Leavitt and Naomi Saphra},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    pages={1--32},
}
@article{HeadCooperation_24_arXiv_UoM,
  title={A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive Language Models},
  author={Kim, Geonhee and Valentino, Marco and Freitas, Andr{\'e}},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2408.08590}{https://doi.org/10.48550/arXiv.2408.08590}},
}
@inproceedings{InductionHead_24_ICML_MIT,
  title = 	 {In-Context Language Learning: Architectures and Algorithms},
  author =       {Aky\"{u}rek, Ekin and Wang, Bailin and Kim, Yoon and Andreas, Jacob},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {787--812},
  year = 	 {2024},
  volume = 	 {235},
  publisher =    {PMLR},
}
@inproceedings{MechanCompet_24_ACL_ETH,
    title = "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals",
    author = {Ortu, Francesco and Jin, Zhijing and Doimo, Diego and Sachan, Mrinmaya and Cazzaniga, Alberto and Sch{\"o}lkopf, Bernhard},
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    publisher = "Association for Computational Linguistics",
    pages = "8420--8436",
}
@article{LoFiT_24_arXiv_UT,
  title={LoFiT: Localized Fine-tuning on LLM Representations},
  author={Yin, Fangcong and Ye, Xi and Durrett, Greg},
  journal={Preprint at arXiv},
  year={Preprint at arXiv},
  note={\href{https://doi.org/10.48550/arXiv.2406.01563}{https://doi.org/10.48550/arXiv.2406.01563}},
}
@article{ConceptDepth_24_arXiv_Rutgers,
  title={Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?},
  author={Jin, Mingyu and Yu, Qinkai and Huang, Jingyuan and Zeng, Qingcheng and Wang, Zhenting and Hua, Wenyue and Zhao, Haiyan and Mei, Kai and Meng, Yanda and Ding, Kaize and others},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2404.07066}{https://doi.org/10.48550/arXiv.2404.07066}}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% 2024/11/20 new to add %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{NgramICLD_24_NIPS_Yale,
  title={Unveiling induction heads: Provable training dynamics and feature learning in transformers},
  author={Chen, Siyu and Sheen, Heejune and Wang, Tianhao and Yang, Zhuoran},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2409.10559}{https://doi.org/10.48550/arXiv.2409.10559}}
}
@article{CoTHead_24_arXiv_India,
  title={How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning},
  author={Dutta, Subhabrata and Singh, Joykirat and Chakrabarti, Soumen and Chakraborty, Tanmoy},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2402.18312}{https://doi.org/10.48550/arXiv.2402.18312}}
}
@article{ArithmeticHead_24_arXiv_Alibaba,
  title={Interpreting and Improving Large Language Models in Arithmetic Calculation},
  author={Zhang, Wei and Wan, Chaoqun and Zhang, Yonggang and Cheung, Yiu-ming and Tian, Xinmei and Shen, Xu and Ye, Jieping},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2409.01659}{https://doi.org/10.48550/arXiv.2409.01659}}
}
@article{ForerunnerHeadICL_24_arXiv_Japan,
  title={Revisiting In-context Learning Inference Circuit in Large Language Models},
  author={Cho, Hakaze and Kato, Mariko and Sakai, Yoshihiro and Inoue, Naoya},
  journal={Preprint at arXiv},
  year={2024}
}
@article{RoPEHead_24_arXiv_Oxford,
  title={Round and Round We Go! What makes Rotary Positional Encodings useful?},
  author={Barbero, Federico and Vitvitskyi, Alex and Perivolaropoulos, Christos and Pascanu, Razvan and Veli{\v{c}}kovi{\'c}, Petar},
  journal={Preprint at arXiv},
  year={2024}
}
@article{LanguageHead_24_arXiv_BrownU,
  title={The Same But Different: Structural Similarities and Differences in Multilingual Language Modeling},
  author={Zhang, Ruochen and Yu, Qinan and Zang, Matianyu and Eickhoff, Carsten and Pavlick, Ellie},
  journal={Preprint at arXiv},
  year={2024}
}
@article{LongContextHead_24_arXiv_MIT,
  title={DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads},
  author={Xiao, Guangxuan and Tang, Jiaming and Zuo, Jingwei and Guo, Junxian and Yang, Shang and Tang, Haotian and Fu, Yao and Han, Song},
  journal={Preprint at arXiv},
  year={2024}
}
@article{SafetyHead_24_arXiv_Westlake,
  title={Locking down the finetuned llms safety},
  author={Zhu, Minjun and Yang, Linyi and Wei, Yifan and Zhang, Ningyu and Zhang, Yue},
  journal={Preprint at arXiv},
  year={2024}
}
@article{SafetyHead_24_arXiv_Alibaba,
  title={On the Role of Attention Heads in Large Language Model Safety},
  author={Zhou, Zhenhong and Yu, Haiyang and Zhang, Xinghua and Xu, Rongwu and Huang, Fei and Wang, Kun and Liu, Yang and Fang, Junfeng and Li, Yongbin},
  journal={Preprint at arXiv},
  year={2024}
}
@article{ActiveDormantHead_24_arXiv_UCB,
  title={Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs},
  author={Guo, Tianyu and Pai, Druv and Bai, Yu and Jiao, Jiantao and Jordan, Michael I and Mei, Song},
  journal={Preprint at arXiv},
  year={2024}
}
@article{SubjectHead_24_arXiv_UMaryland,
  title={A Psycholinguistic Evaluation of Language Models' Sensitivity to Argument Roles},
  author={Lee, Eun-Kyoung Rosa and Nair, Sathvik and Feldman, Naomi},
  journal={Preprint at arXiv},
  year={2024}
}
@article{ArithmeticHead_24_arXiv_IIT,
  title={Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics},
  author={Nikankin, Yaniv and Reusch, Anja and Mueller, Aaron and Belinkov, Yonatan},
  journal={Preprint at arXiv},
  year={2024}
}
@article{MarkovInductionHead_24_arXiv_UCB,
  title={Transformers on markov data: Constant depth suffices},
  author={Rajaraman, Nived and Bondaschi, Marco and Ramchandran, Kannan and Gastpar, Michael and Makkuva, Ashok Vardhan},
  journal={Preprint at arXiv},
  year={2024}
}
@article{CausalGradient_24_ICML_Princeton,
  title={How transformers learn causal structure with gradient descent},
  author={Nichani, Eshaan and Damian, Alex and Lee, Jason D},
  journal={Preprint at arXiv},
  year={2024}
}
@article{ImportantHead_24_arXiv_IBM,
  title={Attention Tracker: Detecting Prompt Injection Attacks in LLMs},
  author={Hung, Kuo-Han and Ko, Ching-Yun and Rawat, Ambrish and Chung, I and Hsu, Winston H and Chen, Pin-Yu and others},
  journal={Preprint at arXiv},
  year={2024}
}
@inproceedings{DetectionHead_24_EMNLP_Oxford,
  title={Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models},
  author={Lan, Michael and Torr, Philip and Barez, Fazl},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={12576--12601},
  year={2024}
}
@article{SEEKER_24_EMNLP_IA,
  title={SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models},
  author={He, Jinghan and Guo, Haiyun and Zhu, Kuan and Zhao, Zihan and Tang, Ming and Wang, Jinqiao},
  journal={Preprint at arXiv},
  year={2024}
}
@article{PropositionLogic_24_arXiv_Purdue,
  title={How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis},
  author={Hong, Guan Zhe and Dikkala, Nishanth and Luo, Enming and Rashtchian, Cyrus and Panigrahy, Rina},
  journal={Preprint at arXiv},
  year={2024}
}
@article{MemoryMath_24_arXiv_MILES,
  title={Memorization in Attention-only Transformers},
  author={Dana, L{\'e}o and Pydi, Muni Sreenivas and Chevaleyre, Yann},
  journal={Preprint at arXiv},
  year={2024},
  note={\href{https://doi.org/10.48550/arXiv.2411.10115}{https://doi.org/10.48550/arXiv.2411.10115}},
}